{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4 - Polynomial questions",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "fFdlKccv3BA4",
        "outputId": "d473aa86-5bc0-4968-97af-908daadf9fd1"
      },
      "source": [
        "#!pip install pycountry_convert\n",
        "#!pip uninstall scikit-learn -y\n",
        "#!pip install scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjr6KMKDz_MI"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "suic_df = pd.read_csv(\"master.csv\")\n",
        "bike_df = pd.read_csv(\"day.csv\")\n",
        "vid_df = pd.read_csv(\"transcoding_mesurment.tsv\", delimiter='\\t')\n",
        "\n",
        "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "##### Suicide Dataset #####\n",
        "def country_to_continent(country_name):\n",
        "    try:\n",
        "        item = country_alpha2_to_continent_code(country_name_to_country_alpha2(country_name))\n",
        "        \n",
        "    #Handle countries not in pycountry\n",
        "    except KeyError:\n",
        "        if (country_name == \"Republic of Korea\"):\n",
        "            item = 'AS'\n",
        "        elif (country_name == \"Saint Vincent and Grenadines\"):\n",
        "            item = 'SA'\n",
        "        else:\n",
        "            print(country_name)\n",
        "    return item\n",
        "\n",
        "\n",
        "suic_df_preprocessed = suic_df.copy()\n",
        "\n",
        "#Convert countries to a continent code\n",
        "suic_df_preprocessed[\"country\"] = suic_df_preprocessed[\"country\"].apply(lambda x: country_to_continent(x))\n",
        "\n",
        "#Remove country-year feature. Also remove target variables\n",
        "suic_df_preprocessed = suic_df_preprocessed.drop(['country-year', 'suicides/100k pop', 'suicides_no'], axis=1)\n",
        "\n",
        "#Necessary because values with commas can't be converted to floats\n",
        "suic_df_preprocessed[' gdp_for_year ($) ']=suic_df_preprocessed[' gdp_for_year ($) '].str.replace(',','')\n",
        "\n",
        "preprocess_suic = make_column_transformer(\n",
        "    (StandardScaler(),['year', 'population', 'HDI for year', ' gdp_for_year ($) ', 'gdp_per_capita ($)']),\n",
        "    (OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore'), ['country', 'sex', 'age', 'generation']),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "suic_df_preprocessed = preprocess_suic.fit_transform(suic_df_preprocessed)\n",
        "#print(suic_df_preprocessed)\n",
        "\n",
        "\n",
        "\n",
        "##### Bike Dataset #####\n",
        "bike_df_preprocessed = bike_df.copy()\n",
        "\n",
        "#Remove seemingly useless columns and target variables\n",
        "#also remove atemp, since it's so similar to temp and does not reveal any new information (correlation with temp is 1:1)\n",
        "#Should we one-hot the season and month data columns? They are currently numerical, but that might not make the most \n",
        "#sense\n",
        "\n",
        "bike_df_preprocessed = bike_df_preprocessed.drop(['instant', 'dteday', 'cnt', 'casual', 'registered', 'atemp'], axis=1)\n",
        "\n",
        "preprocess_bike = make_column_transformer(\n",
        "    (StandardScaler(), bike_df_preprocessed.columns.drop(['season', 'mnth'])),\n",
        "    (OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore'), ['season', 'mnth']),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "bike_df_preprocessed = preprocess_bike.fit_transform(bike_df_preprocessed)\n",
        "#print(bike_df_preprocessed)\n",
        "\n",
        "\n",
        "\n",
        "##### Video Dataset #####\n",
        "\n",
        "vid_df_preprocessed = vid_df.copy()\n",
        "\n",
        "#Remove ID column. Carries no important information. Also remove target\n",
        "vid_df_preprocessed = vid_df_preprocessed.drop(['id', 'utime', 'umem'], axis=1)\n",
        "\n",
        "#Preprocess all but utime\n",
        "preprocess_vid = make_column_transformer(\n",
        "    (StandardScaler(), vid_df_preprocessed.columns.drop(['codec', 'o_codec'])),\n",
        "    (OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore'), ['codec', 'o_codec']),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "vid_df_preprocessed = preprocess_vid.fit_transform(vid_df_preprocessed)\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_regression, f_regression, SelectKBest\n",
        "\n",
        "\n",
        "#Currently using ~3/4 of all features from mutual_info_regression - I have no idea how many we should be using\n",
        "\n",
        "fraction_of_total = 1\n",
        "num_feat_suic = int(np.floor(fraction_of_total * suic_df_preprocessed.shape[1]))\n",
        "score_func_suic = mutual_info_regression\n",
        "#score_func_suic = f_regression\n",
        "\n",
        "num_feat_bike = int(np.floor(fraction_of_total * bike_df_preprocessed.shape[1]))\n",
        "score_func_bike = mutual_info_regression\n",
        "#score_func_bike = f_regression\n",
        "\n",
        "num_feat_vid = int(np.floor(fraction_of_total * vid_df_preprocessed.shape[1]))\n",
        "score_func_vid = mutual_info_regression\n",
        "#score_func_vid = f_regression\n",
        "\n",
        "\n",
        "fs_suic = SelectKBest(score_func=score_func_suic, k=num_feat_suic)\n",
        "fs_bike = SelectKBest(score_func=score_func_bike, k=num_feat_bike)\n",
        "fs_vid = SelectKBest(score_func=score_func_vid, k=num_feat_vid)\n",
        "\n",
        "labels_suic = suic_df['suicides/100k pop']\n",
        "labels_bike = bike_df['cnt']\n",
        "labels_vid = vid_df['utime']\n",
        "\n",
        "#Set nan to 0. Can change later\n",
        "df_nans = np.isnan(suic_df_preprocessed)\n",
        "suic_df_preprocessed[df_nans] = 0\n",
        "\n",
        "#Pick top k features\n",
        "suic_data_ready = fs_suic.fit_transform(suic_df_preprocessed, labels_suic)\n",
        "bike_data_ready = fs_bike.fit_transform(bike_df_preprocessed, labels_bike)\n",
        "vid_data_ready = fs_vid.fit_transform(vid_df_preprocessed, labels_vid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf9vuBeb0mUy"
      },
      "source": [
        "#From project 1\n",
        "class EstimatorSelectionHelper:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        if not set(models.keys()).issubset(set(params.keys())):\n",
        "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
        "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(\"Running GridSearchCV for %s.\" % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, refit=refit,\n",
        "                              return_train_score=True)\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs \n",
        "            \n",
        "    def fit_LGB(self, X, y, cv=3, n_jobs=3, feature_name=None, categorical_feature=None, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(\"Running GridSearchCV for %s.\" % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, refit=refit,\n",
        "                              return_train_score=True)\n",
        "            gs.fit(X,y, feature_name=feature_name, categorical_feature=categorical_feature, verbose=verbose)\n",
        "            self.grid_searches[key] = gs  \n",
        "    \n",
        "    def fit_Cat(self, X, y, cv=3, n_jobs=3, cat_features=None, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(\"Running GridSearchCV for %s.\" % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, refit=refit,\n",
        "                              return_train_score=True)\n",
        "            gs.fit(X,y, cat_features=cat_features, verbose=verbose)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params, mean_train_score):\n",
        "            \n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "                 'mean_train_score': mean_train_score\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            mean_train_scores = self.grid_searches[k].cv_results_['mean_train_score']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s, t in zip(params,all_scores,mean_train_scores):\n",
        "                rows.append((row(k, s, p, t)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score', 'mean_train_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9FR6yEI9eI3",
        "outputId": "8f882588-e4aa-453f-e88c-489e9f00a9ad"
      },
      "source": [
        "poly_features_suic = PolynomialFeatures(degree = 2)\n",
        "suic_poly = poly_features_suic.fit_transform(suic_data_ready)\n",
        "poly_features_bike = PolynomialFeatures(degree = 2)\n",
        "bike_poly = poly_features_bike.fit_transform(bike_data_ready)\n",
        "poly_features_vid = PolynomialFeatures(degree = 2)\n",
        "vid_poly = poly_features_vid.fit_transform(vid_data_ready)\n",
        "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
        "\n",
        "#fs_suic = SelectKBest(score_func=f_regression, k=3)\n",
        "#fs_bike = SelectKBest(score_func=f_regression, k=3)\n",
        "#fs_vid = SelectKBest(score_func=f_regression, k=3)\n",
        "\n",
        "#suic_data_ready = fs_suic.fit_transform(suic_poly, labels_suic)\n",
        "#bike_data_ready = fs_bike.fit_transform(bike_df_preprocessed, labels_bike)\n",
        "#vid_data_ready = fs_vid.fit_transform(vid_df_preprocessed, labels_vid)\n",
        "f_bike, p_bike = f_regression(bike_poly, labels_bike)\n",
        "f_suic, p_suic = f_regression(suic_poly,labels_suic)\n",
        "f_vid, p_vid = f_regression(vid_poly, labels_vid)\n",
        "\n",
        "mi_bike=mutual_info_regression(bike_poly, labels_bike)\n",
        "mi_suic=mutual_info_regression(suic_poly, labels_suic)\n",
        "mi_vid=mutual_info_regression(vid_poly, labels_vid)\n",
        "\n",
        "n=3\n",
        "\n",
        "top_n_bike_index = np.argsort(f_bike)[-1*n:]\n",
        "top_n_suic_index = np.argsort(f_suic)[-1*n:]\n",
        "top_n_vid_index = np.argsort(f_vid)[-1*n:]\n",
        "\n",
        "top_n_bike_f = []\n",
        "for i in top_n_bike_index:\n",
        "  top_n_bike_f.append(poly_features_bike.get_feature_names()[i])\n",
        "\n",
        "top_n_suic_f = []\n",
        "for i in top_n_suic_index:\n",
        "  top_n_suic_f.append(poly_features_suic.get_feature_names()[i])\n",
        "\n",
        "top_n_vid_f = []\n",
        "for i in top_n_vid_index:\n",
        "  top_n_vid_f.append(poly_features_vid.get_feature_names()[i])\n",
        "\n",
        "top_n_bike_index = np.argsort(mi_bike)[-1*n:]\n",
        "top_n_suic_index = np.argsort(mi_suic)[-1*n:]\n",
        "top_n_vid_index = np.argsort(mi_vid)[-1*n:]\n",
        "\n",
        "top_n_bike_mi = []\n",
        "for i in top_n_bike_index:\n",
        "  top_n_bike_mi.append(poly_features_bike.get_feature_names()[i])\n",
        "\n",
        "top_n_suic_mi = []\n",
        "for i in top_n_suic_index:\n",
        "  top_n_suic_mi.append(poly_features_suic.get_feature_names()[i])\n",
        "\n",
        "top_n_vid_mi = []\n",
        "for i in top_n_vid_index:\n",
        "  top_n_vid_mi.append(poly_features_vid.get_feature_names()[i])\n",
        "\n",
        "print(top_n_bike_f)\n",
        "print(top_n_bike_mi)\n",
        "print(top_n_suic_f)\n",
        "print(top_n_suic_mi)\n",
        "print(top_n_vid_f)\n",
        "print(top_n_vid_mi)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:306: RuntimeWarning: invalid value encountered in true_divide\n",
            "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:306: RuntimeWarning: invalid value encountered in true_divide\n",
            "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
            "  corr /= X_norms\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:306: RuntimeWarning: invalid value encountered in true_divide\n",
            "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['x10 x23', 'x10 x21', '1']\n",
            "['x0 x5', 'x5', 'x1 x5']\n",
            "['x6 x9', 'x14 x17', '1']\n",
            "['x1^2', 'x1 x11', 'x1']\n",
            "['x17 x18', 'x17 x20', '1']\n",
            "['x9 x15', 'x3 x16', 'x3 x15']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvw3XH2hERbH",
        "outputId": "5fe435c4-db5e-419b-cc78-97426e86ef64"
      },
      "source": [
        "from copy import copy\n",
        "\n",
        "bike_cols = copy(list(bike_df.columns))\n",
        "#we dropped 'instant', 'dteday', 'cnt', 'casual', 'registered', 'atemp'\n",
        "for i in ['instant', 'dteday', 'cnt', 'casual', 'registered', 'atemp']:\n",
        "  bike_cols.remove(i)\n",
        "\n",
        "\n",
        "suic_cols = copy(list(suic_df.columns))\n",
        "#we dropped 'country-year', 'suicides/100k pop', 'suicides_no'\n",
        "for i in ['country-year', 'suicides/100k pop', 'suicides_no']:\n",
        "  suic_cols.remove(i)\n",
        "\n",
        "vid_cols = copy(list(vid_df.columns))\n",
        "#we dropped 'id', 'utime', 'umem'\n",
        "for i in ['id', 'utime', 'umem']:\n",
        "  vid_cols.remove(i)\n",
        "\n",
        "print(bike_cols)\n",
        "print(suic_cols)\n",
        "print(vid_cols)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed']\n",
            "['country', 'year', 'sex', 'age', 'population', 'HDI for year', ' gdp_for_year ($) ', 'gdp_per_capita ($)', 'generation']\n",
            "['duration', 'codec', 'width', 'height', 'bitrate', 'framerate', 'i', 'p', 'b', 'frames', 'i_size', 'p_size', 'b_size', 'size', 'o_codec', 'o_bitrate', 'o_framerate', 'o_width', 'o_height']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1On7n9fRDY6R"
      },
      "source": [
        "#Q 15\n",
        "l1_classifier = Lasso(max_iter=10000)\n",
        "l2_classifier = Ridge(max_iter=10000)\n",
        "\n",
        "params = {\n",
        "    'alpha': [0.1, 0.01,.001]\n",
        "}\n",
        "\n",
        "models_in = {\n",
        "    'Lasso': l1_classifier,\n",
        "    'Ridge': l2_classifier\n",
        "}\n",
        "\n",
        "params_in = {\n",
        "    'Lasso': params,\n",
        "    'Ridge': params,\n",
        "}\n",
        "\n",
        "poly_features = PolynomialFeatures(degree = 2)\n",
        "suic_poly = poly_features.fit_transform(bike_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(suic_poly, labels_suic, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtpNGPI-Dfxv"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 3)\n",
        "suic_poly = poly_features.fit_transform(suic_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(suic_poly, labels_suic, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6oPX4GDw8g"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 4)\n",
        "suic_poly = poly_features.fit_transform(suic_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(suic_poly, labels_suic, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzLoLSrfDz63"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 2)\n",
        "bike_poly = poly_features.fit_transform(bike_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(bike_poly, labels_bike, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqGUkHfaD7zI"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 3)\n",
        "bike_poly = poly_features.fit_transform(bike_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(bike_poly, labels_bike, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Obvyn5D99Y"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 4)\n",
        "bike_poly = poly_features.fit_transform(bike_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(bike_poly, labels_bike, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O-RQGu3EBX3"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 2)\n",
        "vid_poly = poly_features.fit_transform(vid_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(vid_poly, labels_vid, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMfBkK1EH-I"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 3)\n",
        "vid_poly = poly_features.fit_transform(vid_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(vid_poly, labels_vid, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKBjQr3YEJ9_"
      },
      "source": [
        "poly_features = PolynomialFeatures(degree = 4)\n",
        "vid_poly = poly_features.fit_transform(vid_data_ready)\n",
        "grid_search = EstimatorSelectionHelper(models_in, params_in)\n",
        "grid_search.fit(vid_poly, labels_vid, n_jobs=-1, cv=5,scoring='neg_root_mean_squared_error')\n",
        "grid_search.score_summary(sort_by='mean_score')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}